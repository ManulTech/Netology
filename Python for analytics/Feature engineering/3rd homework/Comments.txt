Привет!

Спасибо за проделанную работу.

Хорошо, что вы применили кодирование к различным признакам и разобрались с возрастом животного. Отмечу, что вы пытались предсказать все 8 возможных исходов судьбы животного из шелтера, хотя для задания нужны были только Transfer и Adopted — не исключено, что если оставить только эти две категории, то качество вырастет.
Не очень поняла, почему вы использовали MinMaxScaler для категориальных данных (окрас и порода) — не сказать, что они от этого испортятся, но сильно это им навряд ли помогло.

Что касается PCA — можно было «поиграться» с ним и посмотреть, поможет ли сжатие размерности (которая сильно выросла из-за one-hot encoding) получить некие гибридные признаки, которые помогут хорошо предсказать результат.

Постараюсь выложить/прислать свой вариант решения. В любом случае, качество модели всё же сравнительно хорошее.
Задание засчитано. Надеюсь, что эти знания вам пригодятся. Удачи!

====
Дарья, добрый вечер! Спасибо за обратную связь по https://github.com/ManulTech/Netology/blob/master/Python%20for%20analytics/Feature%20engineering/Second%20homework/Untitled.ipynb
У меня осталось несколько белых пятен.
"Не очень поняла, почему вы использовали MinMaxScaler для категориальных данных (окрас и порода) — не сказать, что они от этого испортятся, но сильно это им навряд ли помогло." Я его использовал, чтобы снизить размерность. Уточните, пожалуйста,
когда действительно нужно его использовать
нужно ли тут было использовать standard scaler
нужно ли вообще в этой задаче использовать нормирование
2. "Что касается PCA — можно было «поиграться» с ним и посмотреть, поможет ли сжатие размерности"
когда я использую PCA, должен ли я уменьшать размерность для всех данных ВМЕСТЕ с теми, которые хочу предсказать или нужно перед PCA дропнуть значения, которые я предсказываю? Алгоритм, как я понимаю должен быть таким:
сохранить где-то отдельно, а затем дропнуть предсказанные данные из основного df
уменьшить размерность df (данные дропнуты) через pca. Получить pca_df
добавить в получившийся pca_df значения из пункта один
Правильно я понял?
3. "Постараюсь выложить/прислать свой вариант решения" Да, жду решения! Может быть кто-то из участников уже справился и вам понравилась его работа. Пошарьте, пожалуйста

И ещё вопрос. Вы писали, что нужно предсказать только 2 атрибута из 8. Значит ли это, что все данные по остальным 6 атрибутам можно было дропнуть? Или нужно было их принудительно заменить нулями после преобразования через get_dummies? А единицы оставить только в двух предсказываемых
====

Здравствуйте, Александр!
Отвечу на то, что могу сказать быстро
Про PCA: да, ход действий верный! ведь получается, что наш итоговый массив признаков X очень многоразмерный (и мы только раздуваем его ещё больше за счёт one hot encoding и других техник, добавляющих признаки). PCA может позволить нам от каких-то размерностей избавиться
Про атрибуты: да, оставшиеся 6 можно было просто дропнуть) Если посмотреть на распределение вариантов outcome_type, то там становится ясно, что Transfer и Adoption — это два самых частотных результата, которые есть, так что датасет ужимается не так сильно, как можно было бы подумать, с 78 тысяч до примерно 53 (не помню точную цифру)
Про скейлинг и примеры возьму паузу, чтобы ответить не на ходу, но сегодня напишу обязательно :sweat_smile: